---
title: Revealing the Impact of Pre-training Data on Medical Foundation Models
authors:
- Y Zhou
- Z Wang
- Y Wu
- AY Ong
- S Wagner
- E Ruffell
- M Chia
- Z Guan
- L Ju
- J Engelmann
- D Merle
- T Li
- J Shu
- P Nderitu
- K Zou
- JHL Goh
- Q Hou
- X Liu
- Y Wang
- YC Tham
- A Altmann
- C Cheung
- D Alexander
- E Topol
- A Denniston
- TY Wong
- B Sheng
- PA Keane
date: '2025-04-01'
publishDate: '2025-05-25T17:04:11.988116Z'
publication_types:
- manuscript
doi: 10.21203/rs.3.rs-6080254/v1
abstract: <title>Abstract</title> <p>Medical foundation models (FM), pre-trained on
  large-scale unlabelled data, have demonstrated robust performance and high efficiency
  when fine-tuned to various clinically relevant applications. However, the impact
  of pre-training data on medical FM performance such as generalisability and fairness,
  which form the foundation in fine-tuned models, remains unexplored. To address this,
  we sampled two large cohorts from two sites, Moorfields Eye Hospital (UK) and the
  Shanghai Diabetes Prevention Program (China), each containing 904,170 retinal images
  for FM pre-training. We developed parallel FMs using identical processes and compared
  their fairness and generalisability on downstream tasks with publicly available
  datasets and held-out data from each site. Our results demonstrate that, despite
  strong generalisability, medical FMs perform significantly better on downstream
  data that align with the pre-training data in approximately one-third of tasks.
  Additionally, age is a key metadata factor impacting FM fairness and generalisability
  in retinal images, whereas sex and ethnicity show no such impact. These findings
  advocate for an evidence-based approach to pre-training data selection and highlight
  the importance of transparency even for pre-training data, ultimately enhancing
  FM capabilities and guiding FM development and customised application in healthcare.</p>
links:
- name: URL
  url: https://doi.org/10.21203/rs.3.rs-6080254/v1
---
